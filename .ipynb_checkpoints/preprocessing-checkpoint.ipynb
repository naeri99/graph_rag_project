{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8329bfad-b707-465a-a9a3-c036ba40e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict,Tuple\n",
    "import json\n",
    "import requests\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c2da7b-3e9d-4dc7-940f-dff347d99874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸  Starting to clear all graph data...\n",
      "   Deleting all relationships...\n",
      "   Deleting all nodes...\n",
      "   Cleaning up GDS graphs...\n",
      "   - No 'entity' graph to drop\n",
      "   Dropping all indexes...\n",
      "   Dropping all constraints...\n",
      "âœ… Graph cleanup completed!\n",
      "   - Deleted 956 relationships\n",
      "   - Deleted 134 nodes\n",
      "   - Remaining nodes: 0\n"
     ]
    }
   ],
   "source": [
    "stats = tools.clear_all_graph_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a13795-05e2-4e13-a08e-fb9963682e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/1727/pg1727.txt\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b42200c-8f7e-400d-bf95-c6ed57228177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_into_books(text: str) -> List[str]:\n",
    "    return (\n",
    "        text.split(\"PREFACE TO FIRST EDITION\")[2]\n",
    "        .split(\"FOOTNOTES\")[0]\n",
    "        .strip()\n",
    "        .split(\"\\nBOOK\")[1:]\n",
    "    )\n",
    "\n",
    "books = chunk_into_books(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64df7bc4-5552-4018-8b2f-e9c41c12d2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 books with token sizes:\n",
      "- avg 6515.208333333333\n",
      "- min 4459\n",
      "- max 10760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_count = [num_tokens_from_string(el) for el in books]\n",
    "print(\n",
    "    f\"\"\"There are {len(token_count)} books with token sizes:\n",
    "- avg {sum(token_count) / len(token_count)}\n",
    "- min {min(token_count)}\n",
    "- max {max(token_count)}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad22e014-0f51-4250-8dbd-923ded81ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_books = [chunk_text(book, 1000, 40) for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e1059d-a823-4084-b3f3-0afc6e549658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì´ë¯¸ ì •í•´ì ¸ ìžˆëŠ” entity\n",
    "ENTITY_TYPES = \"PERSON,ORGANIZATION,LOCATION,GOD,EVENT,CREATURE,WEAPON_OR_TOOL\"\n",
    "\n",
    "def extract_entities(text: str) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from text using Bedrock\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (entities, relationships) as lists of dictionaries\n",
    "    \"\"\"\n",
    "    # Construct prompt\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": tools.create_extraction_prompt(ENTITY_TYPES, text)}\n",
    "    ]\n",
    "    \n",
    "    # Make the LLM call using Bedrock (model parameter is ignored)\n",
    "    output = tools.chat(messages, model=\"gpt-4o\")  # Will use Bedrock instead\n",
    "    \n",
    "    # Parse output and return both entities and relationships\n",
    "    return tools.parse_extraction_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06963cc-6ead-42a7-8a74-6908c055953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nodes = [] \n",
    "final_relationship = [] \n",
    "number_of_books = 1\n",
    "for book_i, book in enumerate(\n",
    "    tqdm(chunked_books[:number_of_books], desc=\"Processing Books\")\n",
    "):\n",
    "    for chunk_i, chunk in enumerate(tqdm(book, desc=f\"Book {book_i}\", leave=False)):\n",
    "        nodes, relationships = extract_entities(chunk)\n",
    "        final_nodes.extend(nodes)\n",
    "        final_relationship.extend(relationships)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
